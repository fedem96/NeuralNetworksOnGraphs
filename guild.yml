########## Planetoid Inductive ##########
- model: Planetoid-i|planetoid-i
  description: Revisiting Semi-Supervised Learning with Graph Embeddings (Yang et al., 2016)
  operations:

    train:
      description: Train Planetoid-I with hyperparameters reported in the paper
      main: Planetoid/train
      sourcecode: 'nothing'
      flags-dest: args
      flags:
        modality: 
          default: "I"

    test:
      description: Test Planetoid; insert the checkpoint to the model (checkpoint-path)
      main: Planetoid/test
      sourcecode: 'nothing'
      flags-dest: args
      flags:
        modality: 
          default: "I"
        checkpoint-path:
          required: True

    evaluate10:
      description: ... # TODO:
      sourcecode: 'Planetoid/*.py'
      steps:
        - run: train dataset=[citeseer,cora,pubmed] epochs=100000 patience=5000 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train dataset=[citeseer,cora,pubmed] epochs=100000 patience=5000 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
   

    evaluate100:
      description: ... # TODO:
      sourcecode: 'Planetoid/*.py'
      steps:

        - run: train dataset=[citeseer,cora,pubmed] epochs=100000 patience=5000 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train dataset=[citeseer,cora,pubmed] epochs=100000 patience=5000 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
 


########## Planetoid Transductive ##########
- model: Planetoid-t|planetoid-t
  description: Revisiting Semi-Supervised Learning with Graph Embeddings (Yang et al., 2016)
  operations:

    train:
      description: Train Planetoid-T with hyperparameters reported in the paper
      main: Planetoid/train
      sourcecode: 'nothing'
      flags-dest: args
      flags:
        modality: 
          default: "T"
        learning-rate-unsupervised: 
          default: 0.01
        pretrain-batch: 
          default: 2070
        sample-context-rate:
          default: 0.033
        unsupervised-batch: 
          default: 0
        unsupervised-batch-size: 
          default: 200

    test:
      description: Test Planetoid; insert the checkpoint to the model (checkpoint-path)
      main: Planetoid/test
      sourcecode: 'nothing'
      flags-dest: args
      flags:
        modality: 
          default: "T"
        learning-rate-unsupervised: 
          default: 0.01
        pretrain-batch: 
          default: 2070
        sample-context-rate:
          default: 0.033
        unsupervised-batch: 
          default: 0
        unsupervised-batch-size: 
          default: 200
        checkpoint-path:
          required: True

    evaluate10:
      description: ... # TODO:
      sourcecode: 'Planetoid/*.py'
      steps:
        - run: train dataset=[citeseer,cora,pubmed] epochs=100000 patience=5000 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train dataset=[citeseer,cora,pubmed] epochs=100000 patience=5000 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
   
    evaluate100:
      description: ... # TODO:
      sourcecode: 'Planetoid/*.py'
      steps:
        - run: train dataset=[citeseer,cora,pubmed] epochs=100000 patience=5000 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train dataset=[citeseer,cora,pubmed] epochs=100000 patience=5000 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
  


########## ChebNet ##########
- model: ChebNet|chebnet
  description: Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering (Deferrard et al, 2016)

  operations:

    train:
      description: 
      main: ChebNet/train
      sourcecode: 'nothing'
      flags-dest: args

    train-citeseer:
      description: Train ChebNet on citeseer dataset with hyperparameters reported in GCN paper
      main: ChebNet/train
      sourcecode: 'nothing'
      flags-dest: args
      flags:
        dataset:
          default: citeseer
        num-polynomials:
          default: 4

    train-cora:
      description: Train ChebNet on cora dataset with hyperparameters reported in GCN paper
      main: ChebNet/train
      sourcecode: 'nothing'
      flags-dest: args
      flags:
        dataset:
          default: cora
        num-polynomials:
          default: 3

    train-pubmed:
      description: Train ChebNet on pubmed dataset with hyperparameters reported in GCN paper
      main: ChebNet/train
      sourcecode: 'nothing'
      flags-dest: args
      flags:
        dataset:
          default: pubmed
        num-polynomials:
          default: 4

    test: # TODO: finish
      description: 
      main: ChebNet/test
      sourcecode: 'nothing'
      flags-dest: args

    evaluate10:
      description: ... # TODO:
      sourcecode: 'ChebNet/*.py'
      steps:
        - run: train-citeseer patience=40 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train-cora     patience=40 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train-pubmed   patience=40 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train-citeseer patience=40 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train-cora     patience=40 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train-pubmed   patience=40 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
              # in GCN paper: patience=10

    evaluate100:
      description: ... # TODO:
      sourcecode: 'ChebNet/*.py'
      steps:
        - run: train-citeseer patience=40 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train-cora     patience=40 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train-pubmed   patience=40 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train-citeseer patience=40 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train-cora     patience=40 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train-pubmed   patience=40 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
              # in GCN paper: patience=10


########## GCN ##########
- model: GCN|gcn
  description: Semi-Supervised Classification with Graph Convolutional Netowrks (Kipf and welling, 2017)

  operations:

    train:
      description: 
      main: GCN/train
      sourcecode: 'nothing'
      flags-dest: args

    test: # TODO: finish
      description: 
      main: GCN/test
      sourcecode: 'nothing'
      flags-dest: args

    evaluate10:
      description: ... # TODO:
      sourcecode: 'GCN/*.py'
      steps:
        - run: train dataset=[citeseer,cora,pubmed] patience=40 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train dataset=[citeseer,cora,pubmed] patience=40 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
                                    # in GCN paper: patience=10

    evaluate100:
      description: ... # TODO:
      sourcecode: 'GCN/*.py'
      steps:
        - run: train dataset=[citeseer,cora,pubmed] patience=40 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train dataset=[citeseer,cora,pubmed] patience=40 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
                                    # in GCN paper: patience=10



########## GAT ##########
- model: GAT|gat
  description: Graph Attention Networks ( Velickovic et al., 2018)

  operations:

    train:
      description: 
      main: GAT/train
      sourcecode: 'nothing'
      flags-dest: args

    train-citeseer:
      description: Train GAT on citeseer dataset with hyperparameters reported in the paper
      main: GAT/train
      sourcecode: 'nothing'
      flags-dest: args
      flags:
        dataset:
          default: citeseer

    train-cora:
      description: Train GAT on cora dataset with hyperparameters reported in the paper
      main: GAT/train
      sourcecode: 'nothing'
      flags-dest: args
      flags:
        dataset:
          default: cora

    train-pubmed:
      description: Train GAT on pubmed dataset with hyperparameters reported in the paper
      main: GAT/train
      sourcecode: 'nothing'
      flags-dest: args
      flags:
        dataset:
          default: pubmed
        nheads:
          default: 8,8
        learning-rate:
          default: 0.01
        l2-weight:
          default: 0.001        
    
    test:
      description: 
      main: GAT/test
      sourcecode: 'nothing'
      flags-dest: args
      flags:
        checkpoint-path:
          required: True
    
    evaluate10:
      description: ... # TODO:
      sourcecode: 'GAT/*.py'
      steps:
        - run: train-citeseer epochs=1000 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train-cora     epochs=1000 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train-pubmed   epochs=1000 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train-citeseer epochs=1000 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train-cora     epochs=1000 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
        - run: train-pubmed   epochs=1000 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473]
 
    evaluate100:
      description: ... # TODO:
      sourcecode: 'GAT/*.py'
      steps:
        - run: train-citeseer epochs=1000 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train-cora     epochs=1000 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train-pubmed   epochs=1000 verbose=0 net-seed=5687 data-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train-citeseer epochs=1000 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train-cora     epochs=1000 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
        - run: train-pubmed   epochs=1000 verbose=0 yang-splits=True data-seed=5687 net-seed=[9516,1353,551,4975,9399,6608,5688,8964,872,6473,3747,7920,8089,3603,8960,516,2616,2674,7135,728,9493,5414,828,1356,8175,2107,9099,7075,4848,5154,5041,5605,2232,191,2668,7086,204,193,4687,7362,865,494,4560,5790,6891,4139,8860,9002,7870,5800,6503,8179,6603,9024,1850,4274,1062,1896,4636,9313,1312,7659,3732,193,6422,9636,6511,9123,3255,8690,1227,2315,8810,7120,2641,9166,4129,7445,4676,162,1514,4088,5130,4182,4215,9841,2140,5012,1868,3082,7932,819,5152,6991,8591,1598,2042,649,5556,6083]
